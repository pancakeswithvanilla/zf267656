import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation
from tensorflow.keras.layers import LeakyReLU, Embedding, Concatenate, Masking
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam, RMSprop
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.losses import binary_crossentropy
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix
import torch.nn as nn
import torch.optim as optim
import torch
from typing import Tuple
from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split
from numpy import zeros
from numpy import ones
from numpy.random import rand
from numpy.random import randn
import matplotlib.pyplot as plt
import os

# Constants
input_dim = 100
latent_dim = 50
batch_size = 64
epochs = 50
n_critic = 5  # Number of critic updates per generator update
gp_weight = 10.0  # Gradient penalty weight
max_value = 269.25

# Utility Functions
def read_signals(file_name):
    signals = []
    with open(file_name, 'r') as file:
        for line in file:
            signal = list(map(float, line.strip().split()))
            signals.append(signal)
    return np.array(signals)

def prepare_data():
    signal_events = read_signals("fragsigev.txt")
    signal_nonevents = read_signals("fragsignonev.txt")
    labels_events = np.ones((len(signal_events), 1))
    labels_nonevents = np.zeros((len(signal_nonevents), 1))
    x = np.vstack((signal_events, signal_nonevents))
    y = np.vstack((labels_events, labels_nonevents))
    return train_test_split(x, y, test_size=0.2, random_state=42)

def normalize_data(data):
    return data / max_value

def denormalize_data(normalized_data):
    return normalized_data * max_value

def evaluate_samples(model, x_data, y_data):
    # Ensure y_data contains both data and labels
    assert x_data.shape[0] == y_data.shape[0], "Mismatch between x_data and y_data samples"
    
    # Extract signals and labels
    x_data_signals = x_data
    x_data_labels = y_data
    
    # Perform predictions
    predictions = model.predict([x_data_signals, x_data_labels])  # Predict if real or fake samples
    
    # Calculate loss
    loss_fn = tf.keras.losses.BinaryCrossentropy()
    loss = loss_fn(x_data_labels, predictions).numpy()
    
    # Calculate accuracy
    predictions_binary = (predictions > 0.5).astype(np.float32)
    accuracy = np.mean(predictions_binary == x_data_labels)
    
    # Metrics for real events vs. real nonevents
    real_event_mask = (x_data_labels == 1)
    real_nonevent_mask = (x_data_labels == 0)
    
    real_event_predictions = predictions_binary[real_event_mask]
    real_nonevent_predictions = predictions_binary[real_nonevent_mask]
    
    # Calculate metrics for real events
    real_event_labels = x_data_labels[real_event_mask]
    real_event_pred_event = np.mean(real_event_predictions == real_event_labels)  # Correctly predicted events
    real_event_pred_nonevent = np.mean(real_event_predictions != real_event_labels)  # Incorrectly predicted events
    
    # Calculate metrics for real nonevents
    real_nonevent_labels = x_data_labels[real_nonevent_mask]
    real_nonevent_pred_nonevent = np.mean(real_nonevent_predictions == real_nonevent_labels)  # Correctly predicted nonevents
    real_nonevent_pred_event = np.mean(real_nonevent_predictions != real_nonevent_labels)  # Incorrectly predicted nonevents
    
    # Print out metrics
    print(f"Overall Loss: {loss:.4f}")
    print(f"Overall Accuracy: {accuracy:.4f}")
    print(f"Real\Fake Event Predicted as Event: {real_event_pred_event:.4f}")
    print(f"Real\Fake Event Predicted as Nonevent: {real_event_pred_nonevent:.4f}")
    print(f"Real\Fake Nonevent Predicted as Nonevent: {real_nonevent_pred_nonevent:.4f}")
    print(f"Real\Fake Nonevent Predicted as Event: {real_nonevent_pred_event:.4f}")



def evaluate_discriminator_performance(discriminator, x_real, y_real, x_fake, y_fake):
    """
    Evaluate the performance of the discriminator by predicting and comparing real vs. fake samples.

    Args:
        discriminator (tf.keras.Model): The trained discriminator model.
        x_real (np.ndarray): Real signal samples.
        y_real (np.ndarray): Labels for real signal samples (event or nonevent).
        x_fake (np.ndarray): Fake signal samples generated by the generator.
        y_fake (np.ndarray): Labels for fake signal samples (event or nonevent).
    """
    # Predict if samples are real or fake
    real_predictions = discriminator.predict([x_real, y_real])
    fake_predictions = discriminator.predict([x_fake, y_fake])

    # Convert predictions to binary (real or fake)
    real_predictions_binary = (real_predictions > 0.5).astype(np.float32)
    fake_predictions_binary = (fake_predictions > 0.5).astype(np.float32)

    # Define masks for real and fake samples
    real_mask = (y_real == 1)
    fake_mask = (y_fake == 0)

    # Metrics for real samples
    real_pred_real = real_predictions_binary[real_mask]  # Predictions for real samples
    real_pred_fake = real_predictions_binary[~real_mask]  # Predictions for fake samples

    # Metrics for fake samples
    fake_pred_real = fake_predictions_binary[real_mask]  # Predictions for real samples classified as fake
    fake_pred_fake = fake_predictions_binary[~real_mask]  # Predictions for fake samples classified as real

    # Calculate how many samples were predicted as fake or real correctly or incorrectly
    real_predicted_fake_as_real = np.sum(real_predictions_binary[real_mask] == 0)
    real_predicted_fake_as_fake = np.sum(real_predictions_binary[real_mask] == 1)
    fake_predicted_real_as_fake = np.sum(fake_predictions_binary[fake_mask] == 1)
    fake_predicted_real_as_real = np.sum(fake_predictions_binary[fake_mask] == 0)

    print(f"Real samples predicted as fake: {real_predicted_fake_as_real}")
    print(f"Real samples predicted as real: {real_predicted_fake_as_fake}")
    print(f"Fake samples predicted as real: {fake_predicted_real_as_real}")
    print(f"Fake samples predicted as fake: {fake_predicted_real_as_fake}")

# Generator
def build_generator():
    noise_input = Input(shape=(latent_dim,))
    label_input = Input(shape=(1,), dtype='int32')
    label_embedding = Flatten()(Embedding(2, latent_dim)(label_input))
    model_input = Concatenate()([noise_input, label_embedding])
    
    x = Dense(256, activation='relu')(model_input)
    x = BatchNormalization(momentum=0.8)(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization(momentum=0.8)(x)
    x = Dense(1024, activation='relu')(x)
    x = BatchNormalization(momentum=0.8)(x)
    x = Dense(input_dim, activation='tanh')(x)  # Output should match data distribution
    
    model = Model([noise_input, label_input], x)
    return model

# Critic
def build_critic():
    signal_input = Input(shape=(input_dim,))
    label_input = Input(shape=(1,), dtype='int32')
    label_embedding = Flatten()(Embedding(2, input_dim)(label_input))
    
    x = Concatenate()([signal_input, label_embedding])
    x = Dense(512)(x)
    x = LeakyReLU(0.2)(x)
    x = Dense(256)(x)
    x = LeakyReLU(0.2)(x)
    x = Dense(1)(x)  # No activation function for WGAN
    
    model = Model([signal_input, label_input], x)
    return model

# Wasserstein loss
def wasserstein_loss(y_true, y_pred):
    return tf.reduce_mean(y_true * y_pred)

# Gradient Penalty
def gradient_penalty(batch_size, real_samples, fake_samples, labels):
    real_samples = tf.cast(real_samples, tf.float32)
    fake_samples = tf.cast(fake_samples, tf.float32)
    alpha = tf.random.normal([batch_size, 1], 0.0, 1.0)
    diff = fake_samples - real_samples
    interpolated = real_samples + alpha * diff
    
    with tf.GradientTape() as gp_tape:
        gp_tape.watch(interpolated)
        pred = critic([interpolated, labels])
    
    grads = gp_tape.gradient(pred, [interpolated])[0]
    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))
    gp = tf.reduce_mean((norm - 1.0) ** 2)
    return gp

# Optimizers and constants
generator = build_generator()
critic = build_critic()
generator_optimizer = RMSprop(learning_rate=0.00005)
critic_optimizer = RMSprop(learning_rate=0.00005)
n_critic = 5  # Number of critic updates per generator update
gp_weight = 10.0  # Gradient penalty weight

@tf.function
def train_step(real_signals, labels):
    batch_size = tf.shape(real_signals)[0]

    # Train the critic
    for _ in range(n_critic):
        noise = tf.random.normal([batch_size, latent_dim])
        with tf.GradientTape() as critic_tape:
            fake_signals = generator([noise, labels], training=True)
            real_output = critic([real_signals, labels], training=True)
            fake_output = critic([fake_signals, labels], training=True)
            gp = gradient_penalty(batch_size, real_signals, fake_signals, labels)
            d_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + gp_weight * gp

        critic_gradients = critic_tape.gradient(d_loss, critic.trainable_variables)
        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))

    # Train the generator
    noise = tf.random.normal([batch_size, latent_dim])
    with tf.GradientTape() as gen_tape:
        fake_signals = generator([noise, labels], training=True)
        fake_output = critic([fake_signals, labels], training=True)
        g_loss = -tf.reduce_mean(fake_output)

    generator_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)
    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))

    return d_loss, g_loss

def train():
    d_losses = []
    g_losses = []
    save_path = 'losses_plot.png'
    plots_directory = "plots/"
    file_path = os.path.join(plots_directory, save_path)

    x_train, x_test, y_train, y_test = prepare_data()
    x_train_normalized = normalize_data(x_train)
    x_test_normalized = normalize_data(x_test)

    generator = build_generator()
    critic = build_critic()

    for epoch in range(epochs):
        for _ in range(x_train_normalized.shape[0] // batch_size):
            idx = np.random.randint(0, x_train_normalized.shape[0], batch_size)
            real_signals, labels = x_train_normalized[idx], y_train[idx]
            d_loss, g_loss = train_step(real_signals, labels)
        
        d_losses.append(d_loss.numpy())
        g_losses.append(g_loss.numpy())
        print(f"Epoch {epoch + 1}/{epochs}, Critic Loss: {d_loss.numpy()}, Generator Loss: {g_loss.numpy()}")

        # Save generated samples and models periodically
        if epoch == epochs -1:
            noise = np.random.normal(0, 1, (16, latent_dim))
            labels = np.random.randint(0, 2, (16, 1))
            generated_samples = generator.predict([noise, labels])
            generated_samples_denorm = denormalize_data(generated_samples)
            save_signals_to_txt(f'generated_samples_epoch_{epoch + 1}.txt', generated_samples_denorm)

            generator.save(f"generator_epoch_{epoch + 1}.h5")
            plot_losses(d_losses, g_losses, file_path)
            
    noise = np.random.normal(0, 1, (x_test.shape[0], latent_dim))
    fake_labels = np.random.randint(0, 2, (x_test.shape[0], 1))
    fake_signals = generator.predict([noise, fake_labels])
    print("Fake and real samples classification")
    evaluate_discriminator_performance(critic, x_test_normalized, y_test, fake_signals, fake_labels)
    print("Real samples predicted as Event/Nonevent")
    evaluate_samples(critic, x_test_normalized, y_test)
    print("Fake samples predicted as Event/Nonevent")
    evaluate_samples(critic, fake_signals, fake_labels)
    plot_losses(d_losses, g_losses, file_path)
# Start training
x_train, x_test, y_train, y_test = prepare_data()
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024).batch(batch_size)
train()



def save_signals_to_txt(file_name, signals):
    """
    Save each signal to a text file, with each signal on a separate line.

    Args:
        file_name (str): The path to the file where signals will be saved.
        signals (np.ndarray): A NumPy array where each row represents a signal.
    """
    with open(file_name, 'w') as file:
        for signal in signals:
            # Convert each signal to a space-separated string and write it to the file
            line = ' '.join(map(str, signal))
            file.write(line + '\n\n')

def plot_losses(d_losses, g_losses, save_path):
    epochs = range(1, len(d_losses) + 1)
    
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, d_losses, label='Discriminator Loss', color='blue')
    plt.plot(epochs, g_losses, label='Generator Loss', color='red')
    
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Discriminator and Generator Losses')
    plt.legend()
    plt.grid(True)
    
    # Save the plot to a file
    plt.savefig(save_path)
    plt.close()

    

